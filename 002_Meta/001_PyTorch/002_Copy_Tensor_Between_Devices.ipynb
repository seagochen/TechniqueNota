{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GPU 与 CPU 的运算对比\n",
    "\n",
    "首先不是所有的电脑都有GPU，我们这里的GPU要强调，必须是 **Nvidia** 家的显卡，所以你无论是Intel的独显，还是AMD家的独显，都没法使用到以下的特性加速你的计算过程，那就更不要提什么核显这种了。\n",
    "\n",
    "GPU相对CPU来说更擅长科学计算，这是因为GPU舍弃，或大大简化了CPU需要负担的复杂任务执行的Control单元，而同时有更多负责加减乘除运算的ALU单元。"
   ],
   "id": "fd1abac0061cf5ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T05:41:03.419576Z",
     "start_time": "2024-06-27T05:41:02.351361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time"
   ],
   "id": "73359ea6e010365a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了更直观对比两者在计算性能上的差异，我们做一个实验。",
   "id": "3dab211764092eca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T05:43:15.709257Z",
     "start_time": "2024-06-27T05:41:03.420583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    # 程序计时开始\n",
    "    time_start = time.time()\n",
    "\n",
    "    tensor1 = torch.randn(100, 1000, 1000)\n",
    "    tensor2 = torch.randn(100, 1000, 1000)\n",
    "\n",
    "    result = tensor1 * tensor2\n",
    "    for i in range(1000):\n",
    "        result = result + tensor1 * tensor2\n",
    "\n",
    "    # 程序片段后插入以下两行\n",
    "    time_end = time.time()\n",
    "    print('Time cost on CPU = %fs' % (time_end - time_start))\n",
    "\n",
    "\n",
    "    # 程序计时开始\n",
    "    time_start = time.time()\n",
    "\n",
    "    tensor1 = torch.randn(100, 1000, 1000).cuda()\n",
    "    tensor2 = torch.randn(100, 1000, 1000).cuda()\n",
    "\n",
    "    result = tensor1 * tensor2\n",
    "    for i in range(1000):\n",
    "        result = result + tensor1 * tensor2\n",
    "\n",
    "    # 程序片段后插入以下两行\n",
    "    time_end = time.time()\n",
    "    print('Time cost on GPU = %fs' % (time_end - time_start))"
   ],
   "id": "888be5c6ad8e49f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost on CPU = 126.208652s\n",
      "Time cost on GPU = 6.073089s\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这是一个程序计时，我们模拟的是两个高维度的张量数据的计算，这种运算规模在亿级以上。\n",
    "\n",
    "# 张量或模型所在的设备位置\n",
    "我们在创建了张量，或者网络模型后，有时候会好奇这些模型、数据存储在哪里，所以可以通过下面这条命令来查看。\n",
    "\n",
    "~~~python\n",
    "tensor.device\n",
    "~~~\n",
    "\n",
    "这对于模型也是一样的。\n",
    "\n",
    "~~~python\n",
    "next(model.parameters()).device\n",
    "~~~\n",
    "\n",
    "我们来看看怎么用的"
   ],
   "id": "785a1bb20d5ee86c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T05:43:20.910996Z",
     "start_time": "2024-06-27T05:43:15.709257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class UserDefinedModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(100, 30)\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.linear(data)\n",
    "\n",
    "\n",
    "tensor1 = torch.randn(100, 1000, 1000)\n",
    "tensor2 = tensor1.cuda()\n",
    "\n",
    "cpu_model = UserDefinedModel()\n",
    "gpu_model = UserDefinedModel().cuda()"
   ],
   "id": "2e9c823669a0225c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "输出结果：",
   "id": "3a066c569550d9e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T05:43:20.919595Z",
     "start_time": "2024-06-27T05:43:20.910996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"tensor on\", tensor1.device)\n",
    "print(\"tensor on\", tensor2.device)\n",
    "\n",
    "print(\"model on\", next(cpu_model.parameters()).device)\n",
    "print(\"model on\", next(gpu_model.parameters()).device)"
   ],
   "id": "f2a7cd7e3a562c09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor on cpu\n",
      "tensor on cuda:0\n",
      "model on cpu\n",
      "model on cuda:0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 把数据或模型从CPU转到GPU上\n",
    "\n",
    "由于 torch 同时支持GPU与CPU计算, 这使得它的应用性可以更广泛的覆盖到不同的设备上. 你可以根据自己设备的特点, 来决定如何更好的使用 torch。\n",
    "\n",
    "目前对于科学计算来说，Nvidia家的显卡能更好的支持科学计算, 所以自然而然的, 当需要消耗大量资源的矩阵计算出现时, 最好是把这类计算全部放到GPU上更为合算. 所以我们可以通过以下命令，把数据写入到GPU的运存上\n",
    "\n",
    "拷贝方法很简单，可以这样做：\n",
    "\n",
    "**CPU $\\rightarrow$ GPU**\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.cuda()\n",
    "```\n",
    "\n",
    "你也可以这样："
   ],
   "id": "389eab46709d3271"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T05:43:20.956346Z",
     "start_time": "2024-06-27T05:43:20.919595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 检查支持什么设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 把数据拷贝到适合的设备上\n",
    "tensor1 = torch.randn(10, 10).to(device)\n",
    "print(tensor1.device)"
   ],
   "id": "30bf287052c27e1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "相对来说，上面的方法相对比较温和一些\n",
    "\n",
    "# 把数据或模型拷贝到多张GPU上\n",
    "\n",
    "如果你有多张GPU，通常是渲染工作站，有多过一张的CUDA卡，那么也可以通过指定的形式，把数据传拷贝到指定的 GPU 设备的运存上\n",
    "\n",
    "```python\n",
    "\ttensor_cuda_0 = tensor.to('cuda:0')\n",
    "\ttensor_cuda_1 = tensor.to('cuda:1')\n",
    "\ttensor_cuda_2 = tensor.to('cuda:2')\n",
    "\t...\n",
    "```\n",
    "\n",
    "也可以使用上面提到过的device方法\n",
    "\n",
    "~~~python\n",
    "import torch\n",
    "\n",
    "# 检查支持什么设备\n",
    "device1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device2 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device3 = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device4 = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "...\n",
    "\n",
    "tensor_cuda_0 = tensor.to(device1)\n",
    "tensor_cuda_1 = tensor.to(device2)\n",
    "tensor_cuda_2 = tensor.to(device3)\n",
    "...\n",
    "~~~\n",
    "\n",
    "或者直接用.cuda()\n",
    "\n",
    "~~~python\n",
    "tensor1 = torch.randn(10, 10).cuda()\n",
    "~~~\n",
    "\n",
    "这个指令会把数据传送到默认的CUDA设备上，如果有多块设备要制定，可以这样\n",
    "\n",
    "~~~python\n",
    "tensor1 = torch.randn(10, 10).cuda(0)\n",
    "tensor2 = torch.randn(10, 10).cuda(1)\n",
    "tensor3 = torch.randn(10, 10).cuda(2)\n",
    "...\n",
    "~~~\n",
    "\n",
    "# 把数据或模型拷贝到回CPU上\n",
    "\n",
    "\n",
    "如果想要把数据从 GPU 拷贝到回 CPU 的运存上，那么就执行如下命令：\n",
    "\n",
    "```python\n",
    "\ttensor = tensor.to('cpu')\n",
    "```\n",
    "\n",
    "除此以外，目前的torch还支持使用 cpu() ，以函数的形式进行拷贝。\n",
    "\n",
    "```python\n",
    "\ttensor = tensor.cpu()\n",
    "```"
   ],
   "id": "27f6bcb7ecfb35b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
